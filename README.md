# Predicting the performance of secure neural network inference

This repository serves as a companion page for the paper titled "Predicting the performance of secure neural network inference," submitted to the 39th International Conference on ICT Systems Security and Privacy Protection (SEC 2024).

In this work, our objective is to develop predictors to estimate the execution time of Secure Neural Network Inference (SNNI). These predictors are intended to enhance the efficiency of SNNI by facilitating the selection of neural network architectures that can be evaluated more rapidly using multi-party computing (MPC) or homomorphic encryption (HE).

For any inquiries, please contact the authors via the provided email addresses.

## Authors (Affiliation: University of Amsterdam)
**Eloise Zhang** (j.zhang7@uva.nl)
**Zoltán Ádám Mann** (z.a.mann@uva.nl)


## Repository Contents
This repository contains all the materials required for replicating the study, including:

- The dataset generated from the experiment that yielded the results presented in our paper.

- The source code required to reproduce the results presented in our paper, utilizing the provided dataset.

- A comprehensive set of additional results complementing those presented in the paper.

<!-- ## How to cite us
The scientific article describing design, execution, and main results of this study is available [here](https://www.google.com).<br> 
If this study is helping your research, consider to cite it is as follows, thanks!

```
@article{,
  title={},
  author={},
  journal={},
  volume={},
  pages={},
  year={},
  publisher={}
}
``` -->

<!-- ## Quick started

- To get started:

  ```
  git clone https://github.com/Ckkk112138/GoGreen2-replication-package.git
  cd GoGreen2-replication-package/
  pip install -r requirements.txt
  ```

- To execute the experiment and generate the dataset in `data/experiment_data`:

  `python src/experiment-runner src/script/linux-powerjoular-profiling/RunnerConfig.py`

- To reproduce the results from the dataset:

  `python data/script/<example_test>`
  Replace `<example_test>` with the actual name of your test file under `data/script` folder. -->
<!-- 
## Repository Structure
This is the root directory of the repository. The directory is structured as follows:

    SNNI-Performance-Evaluator
     .
     |
     |--- src/                             Source code and dependencies used in the paper
            |
            |--- script/                   Code for executing the experiment
     |
     |--- documentation/                   Our paper detailing the experiment settings and results
     |
     |--- data/                            Data used in the paper 
            |
            |--- experiment_data/          Dataset generated from the experiment
            |--- script/                   Script for generating and visualizing the graphs from the dataset
            |--- plot/                     Graphical representation of the results -->
  

<!-- Usually, replication packages should include:
* a [src](src/) folder, containing the entirety of the source code used in the study,
* a [data](data/) folder, containing the raw, intermediate, and final data of the study
* if needed, a [documentation](documentation/) folder, where additional information w.r.t. this README is provided. 

In addition, the replication package can include additional data/results (in form of raw data, tables, and/or diagrams) which were not included in the study manuscript. -->

<!-- ## Replication package naming convention
The final name of this repository, as appearing in the published article, should be formatted according to the following naming convention:
`<short conference/journal name>-<yyyy>-<semantic word>-<semantic word>-rep-pkg`

For example, the repository of a research published at the International conference on ICT for Sustainability (ICT4S) in 2022, which investigates cloud tactics would be named `ICT4S-2022-cloud-tactics-rep-pkg` -->

## License
<!-- As general indication, we suggest to use:
* [MIT license](https://opensource.org/licenses/MIT) for code-based repositories, and 
* [Creative Commons Attribution 4.0	(CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/) for text-based repository (papers, docts, etc.). -->

SNNI-Performance-Evaluator is licensed under the [MIT license](https://opensource.org/licenses/MIT), allowing users to freely use, modify, and distribute the code for various purposes.

<!-- For more information on how to add a license to your replication package, refer to the [official GitHUb documentation](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-license-to-a-repository). -->
